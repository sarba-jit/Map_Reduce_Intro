{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Using Map Reduce\n",
    "\n",
    "We have a very big set of news articles and we want to find the top 10 used words not including stop words. We will use the dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_test = fetch_20newsgroups(subset='train',\n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "data = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each text in the dataset, we want to tokenize it, clean it, remove stop words and finally count the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^\\w\\s]','',word).lower()\n",
    "\n",
    "def word_not_in_stopwords(word):\n",
    "    return word not in text.ENGLISH_STOP_WORDS and word and word.isalpha()\n",
    "    \n",
    "    \n",
    "def find_top_words(data):\n",
    "    cnt = Counter()\n",
    "    for text in data:\n",
    "        tokens_in_text = text.split()\n",
    "        tokens_in_text = map(clean_word, tokens_in_text)\n",
    "        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "        cnt.update(tokens_in_text)\n",
    "        \n",
    "    return cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Without Map Reduce\n",
    "Let’s see how much time does it take without MapReduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 s, sys: 181 µs, total: 2.79 s\n",
      "Wall time: 2.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('x', 5086),\n",
       " ('people', 3996),\n",
       " ('like', 3882),\n",
       " ('dont', 3861),\n",
       " ('just', 3743),\n",
       " ('know', 3477),\n",
       " ('maxaxaxaxaxaxaxaxaxaxaxaxaxaxax', 3307),\n",
       " ('use', 3158),\n",
       " ('think', 2995),\n",
       " ('time', 2799)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_top_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## With Map Reduce\n",
    "\n",
    "Now, let’s write our mapper, reducer and chunk_mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out\n",
    "\n",
    "def mapper(text):\n",
    "    tokens_in_text = text.split()\n",
    "    tokens_in_text = map(clean_word, tokens_in_text)\n",
    "    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "    return Counter(tokens_in_text)\n",
    "\n",
    "def reducer(cnt1, cnt2):\n",
    "    cnt1.update(cnt2)\n",
    "    return cnt1\n",
    "\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(mapper, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the function do ?\n",
    "\n",
    "The mapper gets a text, splits it into tokens, cleans them and filters stop words and non-words, finally, it counts the words within this single text document. The reducer function gets 2 counters and merges them. The chunk_mapper gets a chunk and does a MapReduce on it. Now let’s run using the framework we built it and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('x', 5086), ('people', 3996), ('like', 3882), ('dont', 3861), ('just', 3743), ('know', 3477), ('maxaxaxaxaxaxaxaxaxaxaxaxaxaxax', 3307), ('use', 3158), ('think', 2995), ('time', 2799)]\n",
      "CPU times: user 147 ms, sys: 36.8 ms, total: 184 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "from functools import reduce\n",
    "\n",
    "pool = Pool(6)\n",
    "\n",
    "data_chunks = chunkIt(data, num=6)#step 1:\n",
    "\n",
    "mapped = pool.map(chunk_mapper, data_chunks)#step 2:\n",
    "\n",
    "reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(reduced.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
